{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "msmonai",
   "display_name": "msmonai",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Linear Regression with Pytorch model\n",
    "1. Design model (input, output size, forward pass)\n",
    "2. Construct loss and optimizer\n",
    "3. Training loop\n",
    "    - forward propagation: compute prdiction\n",
    "    - backward propagation: gradients\n",
    "    - update weights"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction before training: f(5) = -2.277\n",
      "epoch 1: w = -0.143, b = 0.376, loss = 43.97863770\n",
      "epoch 11: w = 1.383, b = 0.862, loss = 1.28878391\n",
      "epoch 21: w = 1.636, b = 0.917, loss = 0.17550947\n",
      "epoch 31: w = 1.685, b = 0.903, loss = 0.13843153\n",
      "epoch 41: w = 1.700, b = 0.878, loss = 0.12967908\n",
      "epoch 51: w = 1.710, b = 0.852, loss = 0.12211309\n",
      "epoch 61: w = 1.719, b = 0.827, loss = 0.11500504\n",
      "epoch 71: w = 1.727, b = 0.803, loss = 0.10831112\n",
      "epoch 81: w = 1.735, b = 0.779, loss = 0.10200687\n",
      "epoch 91: w = 1.743, b = 0.756, loss = 0.09606957\n",
      "epoch 101: w = 1.750, b = 0.734, loss = 0.09047784\n",
      "epoch 111: w = 1.758, b = 0.712, loss = 0.08521153\n",
      "epoch 121: w = 1.765, b = 0.691, loss = 0.08025177\n",
      "epoch 131: w = 1.772, b = 0.671, loss = 0.07558069\n",
      "epoch 141: w = 1.779, b = 0.651, loss = 0.07118146\n",
      "epoch 151: w = 1.785, b = 0.632, loss = 0.06703838\n",
      "epoch 161: w = 1.792, b = 0.613, loss = 0.06313640\n",
      "epoch 171: w = 1.798, b = 0.595, loss = 0.05946150\n",
      "epoch 181: w = 1.804, b = 0.577, loss = 0.05600049\n",
      "epoch 191: w = 1.809, b = 0.560, loss = 0.05274102\n",
      "epoch 201: w = 1.815, b = 0.544, loss = 0.04967120\n",
      "epoch 211: w = 1.821, b = 0.528, loss = 0.04678004\n",
      "epoch 221: w = 1.826, b = 0.512, loss = 0.04405724\n",
      "epoch 231: w = 1.831, b = 0.497, loss = 0.04149288\n",
      "epoch 241: w = 1.836, b = 0.482, loss = 0.03907778\n",
      "epoch 251: w = 1.841, b = 0.468, loss = 0.03680325\n",
      "epoch 261: w = 1.846, b = 0.454, loss = 0.03466110\n",
      "epoch 271: w = 1.850, b = 0.441, loss = 0.03264363\n",
      "epoch 281: w = 1.855, b = 0.428, loss = 0.03074363\n",
      "epoch 291: w = 1.859, b = 0.415, loss = 0.02895418\n",
      "epoch 301: w = 1.863, b = 0.403, loss = 0.02726890\n",
      "epoch 311: w = 1.867, b = 0.391, loss = 0.02568169\n",
      "epoch 321: w = 1.871, b = 0.379, loss = 0.02418688\n",
      "epoch 331: w = 1.875, b = 0.368, loss = 0.02277910\n",
      "epoch 341: w = 1.878, b = 0.357, loss = 0.02145321\n",
      "epoch 351: w = 1.882, b = 0.347, loss = 0.02020453\n",
      "epoch 361: w = 1.886, b = 0.337, loss = 0.01902851\n",
      "epoch 371: w = 1.889, b = 0.327, loss = 0.01792095\n",
      "epoch 381: w = 1.892, b = 0.317, loss = 0.01687787\n",
      "epoch 391: w = 1.895, b = 0.308, loss = 0.01589551\n",
      "epoch 401: w = 1.898, b = 0.298, loss = 0.01497031\n",
      "epoch 411: w = 1.901, b = 0.290, loss = 0.01409895\n",
      "epoch 421: w = 1.904, b = 0.281, loss = 0.01327833\n",
      "epoch 431: w = 1.907, b = 0.273, loss = 0.01250544\n",
      "epoch 441: w = 1.910, b = 0.265, loss = 0.01177757\n",
      "epoch 451: w = 1.913, b = 0.257, loss = 0.01109205\n",
      "epoch 461: w = 1.915, b = 0.249, loss = 0.01044643\n",
      "epoch 471: w = 1.918, b = 0.242, loss = 0.00983842\n",
      "epoch 481: w = 1.920, b = 0.235, loss = 0.00926576\n",
      "epoch 491: w = 1.922, b = 0.228, loss = 0.00872645\n",
      "epoch 501: w = 1.925, b = 0.221, loss = 0.00821851\n",
      "epoch 511: w = 1.927, b = 0.215, loss = 0.00774017\n",
      "epoch 521: w = 1.929, b = 0.208, loss = 0.00728965\n",
      "epoch 531: w = 1.931, b = 0.202, loss = 0.00686534\n",
      "epoch 541: w = 1.933, b = 0.196, loss = 0.00646575\n",
      "epoch 551: w = 1.935, b = 0.190, loss = 0.00608941\n",
      "epoch 561: w = 1.937, b = 0.185, loss = 0.00573497\n",
      "epoch 571: w = 1.939, b = 0.179, loss = 0.00540116\n",
      "epoch 581: w = 1.941, b = 0.174, loss = 0.00508679\n",
      "epoch 591: w = 1.943, b = 0.169, loss = 0.00479070\n",
      "epoch 601: w = 1.944, b = 0.164, loss = 0.00451186\n",
      "epoch 611: w = 1.946, b = 0.159, loss = 0.00424923\n",
      "epoch 621: w = 1.948, b = 0.154, loss = 0.00400191\n",
      "epoch 631: w = 1.949, b = 0.150, loss = 0.00376898\n",
      "epoch 641: w = 1.951, b = 0.145, loss = 0.00354961\n",
      "epoch 651: w = 1.952, b = 0.141, loss = 0.00334300\n",
      "epoch 661: w = 1.953, b = 0.137, loss = 0.00314843\n",
      "epoch 671: w = 1.955, b = 0.133, loss = 0.00296516\n",
      "epoch 681: w = 1.956, b = 0.129, loss = 0.00279258\n",
      "epoch 691: w = 1.957, b = 0.125, loss = 0.00263003\n",
      "epoch 701: w = 1.959, b = 0.121, loss = 0.00247696\n",
      "epoch 711: w = 1.960, b = 0.118, loss = 0.00233279\n",
      "epoch 721: w = 1.961, b = 0.114, loss = 0.00219699\n",
      "epoch 731: w = 1.962, b = 0.111, loss = 0.00206912\n",
      "epoch 741: w = 1.963, b = 0.108, loss = 0.00194869\n",
      "epoch 751: w = 1.964, b = 0.105, loss = 0.00183527\n",
      "epoch 761: w = 1.966, b = 0.101, loss = 0.00172845\n",
      "epoch 771: w = 1.967, b = 0.098, loss = 0.00162784\n",
      "epoch 781: w = 1.968, b = 0.096, loss = 0.00153310\n",
      "epoch 791: w = 1.968, b = 0.093, loss = 0.00144386\n",
      "epoch 801: w = 1.969, b = 0.090, loss = 0.00135982\n",
      "epoch 811: w = 1.970, b = 0.087, loss = 0.00128068\n",
      "epoch 821: w = 1.971, b = 0.085, loss = 0.00120613\n",
      "epoch 831: w = 1.972, b = 0.082, loss = 0.00113593\n",
      "epoch 841: w = 1.973, b = 0.080, loss = 0.00106981\n",
      "epoch 851: w = 1.974, b = 0.077, loss = 0.00100755\n",
      "epoch 861: w = 1.974, b = 0.075, loss = 0.00094889\n",
      "epoch 871: w = 1.975, b = 0.073, loss = 0.00089367\n",
      "epoch 881: w = 1.976, b = 0.071, loss = 0.00084165\n",
      "epoch 891: w = 1.977, b = 0.069, loss = 0.00079266\n",
      "epoch 901: w = 1.977, b = 0.067, loss = 0.00074652\n",
      "epoch 911: w = 1.978, b = 0.065, loss = 0.00070307\n",
      "epoch 921: w = 1.979, b = 0.063, loss = 0.00066215\n",
      "epoch 931: w = 1.979, b = 0.061, loss = 0.00062361\n",
      "epoch 941: w = 1.980, b = 0.059, loss = 0.00058731\n",
      "epoch 951: w = 1.980, b = 0.057, loss = 0.00055313\n",
      "epoch 961: w = 1.981, b = 0.056, loss = 0.00052093\n",
      "epoch 971: w = 1.982, b = 0.054, loss = 0.00049061\n",
      "epoch 981: w = 1.982, b = 0.052, loss = 0.00046205\n",
      "epoch 991: w = 1.983, b = 0.051, loss = 0.00043516\n",
      "Prediction after training: f(5) = 9.965\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "# f = w*x\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype = torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype = torch.float32)\n",
    "X_test = torch.tensor([[5]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "#model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "print('Prediction before training: f(5) = {:.3f}'.format(model(X_test).item()))\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 1000\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    # back propagation\n",
    "    \n",
    "    l.backward()\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, b = {b[0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}